{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Starting MNIST from scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOIw8BhRWBOhe3jpfsOkiT8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitudaniel/fastai-course-notes/blob/main/Starting_MNIST_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsYpggNply0A",
        "outputId": "f19901fe-8378-4840-95ef-492e69b7eeec"
      },
      "source": [
        "!pip install fastai --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastai\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/53/edf39e15b7ec5e805a0b6f72adbe48497ebcfa009a245eca7044ae9ee1c6/fastai-2.3.0-py3-none-any.whl (193kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 25.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 51kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 61kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 71kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 81kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 92kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 102kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 112kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 122kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 133kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 143kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 153kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 163kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 174kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 184kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from fastai) (2.23.0)\n",
            "Collecting fastcore<1.4,>=1.3.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/98/60404e2817cff113a6ae4023bc1772e23179408fdf7857fa410551758dfe/fastcore-1.3.19-py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (7.1.2)\n",
            "Collecting torch<1.8,>=1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 22kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: spacy<3 in /usr/local/lib/python3.7/dist-packages (from fastai) (2.2.4)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: pip in /usr/local/lib/python3.7/dist-packages (from fastai) (19.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from fastai) (1.1.5)\n",
            "Collecting torchvision<0.9,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/df/969e69a94cff1c8911acb0688117f95e1915becc1e01c73e7960a2c76ec8/torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 239kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from fastai) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from fastai) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from fastprogress>=0.2.4->fastai) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.8,>=1.7.0->fastai) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (56.0.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->fastai) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3->fastai) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->fastai) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3->fastai) (3.4.1)\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fastcore, torch, torchvision, fastai\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaWd8L9marXI"
      },
      "source": [
        "pip install fastbook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b710nOaXatpu"
      },
      "source": [
        "# import fastbook\n",
        "# fastbook.setup_book()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KS_IIlBa1eW"
      },
      "source": [
        "from fastbook import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXimfQGnm_Sb"
      },
      "source": [
        "import fastai\n",
        "print(fastai.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKX13y3TlSK7"
      },
      "source": [
        "from fastai.vision.all import *\n",
        "#from utils import *\n",
        "\n",
        "matplotlib.rc('image', cmap='Greys')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO738Uc7ppb0"
      },
      "source": [
        "- We're going to start simple and gradually scale up.\n",
        "\n",
        "- Fastai has a simpler version of MNIST with only 3's and 7's"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5HN5DDGlqfr"
      },
      "source": [
        "path = untar_data(URLs.MNIST_SAMPLE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMG4GCLkp3a0"
      },
      "source": [
        "Path.BASE_PATH = path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p3gicwuqJ-k"
      },
      "source": [
        "path.ls()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi372YWXqYtd"
      },
      "source": [
        "- `path.ls()` prints a count of items and a list of files relative to where our MNIST_SAMPLE was untarred.\n",
        "\n",
        "- `path` is a `pathlib` path object. \n",
        "\n",
        "- `Pathlib` is part of the standard library but `ls` is not an inbuild method but a method added by the creators of fastai to make it easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj-b-U-PqOkn"
      },
      "source": [
        "# show what ls is\n",
        "path.ls?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRCTbNfZrFlo"
      },
      "source": [
        "# Show what ls does\n",
        "path.ls??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bec5du5RrIp9"
      },
      "source": [
        "# show the documentation\n",
        "doc(path.ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1CQa_CMrVcn"
      },
      "source": [
        "(path/'train').ls()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Appc-5YErnQ1"
      },
      "source": [
        "- We see that the data in our train folder is labelled as \"3\" or \"7\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3I00T2ErdE3"
      },
      "source": [
        "# sorted() ensures we get the same order of files\n",
        "threes = (path/'train'/'3').ls().sorted()\n",
        "sevens = (path/'train'/'7').ls().sorted()\n",
        "\n",
        "threes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFOE40P2r4kE"
      },
      "source": [
        "im3_path = threes[1]\n",
        "im3 = Image.open(im3_path)\n",
        "im3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPjJVDP8sZCe"
      },
      "source": [
        "type(im3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMkGmJDNse7E"
      },
      "source": [
        "- PIL is the Python Imaging library; the most popular library for working with images on Python by far.\n",
        "\n",
        "- Our image is a Png file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNKGHaHstDvr"
      },
      "source": [
        "- To view the numbers that make up this image, we have to convert it into a *Numpy array* or a *PyTorch tensor*.\n",
        "\n",
        "- A tensor is a PyTorch version of a Numpy array.\n",
        "\n",
        "- A PyTorch tensor and a Numpy array behave nearly identically much if not most of the time.\n",
        "\n",
        "- The key difference is that a PyTorch tensor can also be computed on a GPU and not just a CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr8RYE1qs5cA"
      },
      "source": [
        "# Numbers from the top-left of the image as a Numpy array\n",
        "array(im3)[4:10,4:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tQ_EF-PtYzv"
      },
      "source": [
        "# Numbers from the top-left of the image as a PyTorch tensor\n",
        "tensor(im3)[4:10,4:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOb1V4wTutXo"
      },
      "source": [
        "- We can slice the array to pick just the part with the top part of the digit in it, and use a Pandas DataFrame to color-code the values using a gradient.\n",
        "\n",
        "- This shows us clearly how the image is created from the pixel values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G24vl0POtzaq"
      },
      "source": [
        "im3_t = tensor(im3)\n",
        "df = pd.DataFrame(im3_t[4:15,4:22])\n",
        "df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTGJq0e0vP76"
      },
      "source": [
        "- The background white pixels are stored as the number zero, black is the number 255 and shades of grey are between the two.\n",
        "\n",
        "- The entire image contains 28 pixels across and 28 pixels down, for a total of 768 pixels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hjaS-2qjHrG"
      },
      "source": [
        "# Pixel similarity\n",
        "\n",
        "- As a first try, let's find the average pixel value for every pixel of the threes and do the same for each of the sevens.\n",
        "\n",
        "- This will give us three group averages which will serve as an \"ideal\" 3 and 7.\n",
        "\n",
        "- For classification, we see which of these two \"ideal\" digits the image is most similar to.\n",
        "\n",
        "- This will make a good baseline.\n",
        "\n",
        "- `Baseline` - a super simple model that should be very easy to program from scratch with very little magic and will be better than a random model.\n",
        "\n",
        "- Always start with a reasonable baseline and build on top of it.\n",
        "\n",
        "- Step one of our model is to get the average pixel values for each of out two groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3u8Rzgauqz6"
      },
      "source": [
        "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
        "three_tensors = [tensor(Image.open(o)) for o in threes]\n",
        "len(three_tensors), len(seven_tensors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFoi6YhymCPU"
      },
      "source": [
        "- We check that one of the images looks okay.\n",
        "\n",
        "- Remember that our images are now tensors. Jupyter displays tensors as values since it only displays images by default for PIL objects.\n",
        "\n",
        "- To display the tensor as an image we use fastai's `show_image` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlNYSW2llfWc"
      },
      "source": [
        "show_image(three_tensors[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2iZs5cEmgG1"
      },
      "source": [
        "type(three_tensors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOP4E_Xum08i"
      },
      "source": [
        "three_tensors[1].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owvZFc8Dm4gj"
      },
      "source": [
        "- We need to get the average of all of those threes.\n",
        "\n",
        "- Currently `three_tensors` is a list of 28 x 28 images.\n",
        "\n",
        "- We want to stack all those images into a single three-dimensional tensor to make it easier to perform mathematical computations on it.\n",
        "\n",
        "- PyTorch comes with a function called `stack`.\n",
        "\n",
        "- Some operations in PyTorch, such as taking a mean require us to cast our integer types to float types.\n",
        "\n",
        "- The pixels in an image range from 0 - 255\n",
        "\n",
        "- In computer vision, when working with floats, they are expected to be between zero and one, so we'll divide by 255"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbTZvpV4m3O6"
      },
      "source": [
        "stacked_sevens = torch.stack(seven_tensors).float()/255\n",
        "stacked_threes = torch.stack(three_tensors).float()/255\n",
        "stacked_threes.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VdXbvlio7Oh"
      },
      "source": [
        "- `Rank` - number of axes (dimensions) in a tensor.\n",
        "\n",
        "- `Shape` - size of each axis of a tensor.\n",
        "\n",
        "- `stacked_threes` is now a rank 3 tensor because it has 3 axes.\n",
        "\n",
        "- We get the rank of a tensor by getting the length of its shape or getting the number of dimensions.\n",
        "\n",
        "- We now have 6131 images each of size 28 x 28 pixels.\n",
        "\n",
        "- The semantics of a tensor are entirely up to us and how we construct it. The first axis doesn't have to be the number of images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20Td6qc0ow1r"
      },
      "source": [
        "# length of its shape\n",
        "len(stacked_threes.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P180uQ6Yp849"
      },
      "source": [
        "# number of dimensions\n",
        "stacked_threes.ndim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0seQu0uqpupa"
      },
      "source": [
        "- We can now compute what the ideal three and seven look like.\n",
        "\n",
        "- For every pixel position, we calculate the mean over all images by taking the mean along dimension zero of our stacked, rank-3 tensor.\n",
        "\n",
        "- Dimension zero is the dimension which indexes over all the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcDZ1PesptST"
      },
      "source": [
        "mean3 = stacked_threes.mean(0)\n",
        "show_image(mean3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mlo-1klotoba"
      },
      "source": [
        "mean3.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRADdw0WtyxU"
      },
      "source": [
        "`stacked_threes.mean()` returns the average pixel across the rank-3 tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQSK2rUdtqjt"
      },
      "source": [
        "stacked_threes.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJcdCjnRt8YX"
      },
      "source": [
        "mean7 = stacked_sevens.mean(0)\n",
        "show_image(mean7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1_rb1Z-uVC0"
      },
      "source": [
        "- We can now pick an arbitrary \"3\" and measure its distance from each of these ideal digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNvcfJMQuFpX"
      },
      "source": [
        "a_3 = stacked_threes[1]\n",
        "show_image(a_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jONGq2lXxQ7V"
      },
      "source": [
        "a_7 = stacked_sevens[1]\n",
        "show_image(a_7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRTJsXUfvWXS"
      },
      "source": [
        "- We can't just add up the differences between the pixels of this digit and the ideal digit.\n",
        "\n",
        "- This is because some differences will be positive and others will be negative and these differences cancel out and shows that the image has zero total differences from the ideal.\n",
        "\n",
        "- This is very misleading.\n",
        "\n",
        "- There's two ways data scientists measure distance in this context:\n",
        "  - `L1 norm/Mean Absolute Difference` - Take the mean of the absolute value of differences (replace negatives with positives and get the mean)\n",
        "  - `L2 norm/Root Mean Squared Error (RMSE)` - Take the mean of the square of differences and then take the square root.\n",
        "\n",
        "- Intuitively the difference between `L1 norm` and `mean squared error (MSE)` is that the latter will penalize bigger mistakes more heavily than the former and be more lenient with small mistakes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_CBK7CcvUSK"
      },
      "source": [
        "dist_3_abs = (a_3 - mean3).abs().mean()\n",
        "dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\n",
        "dist_3_abs, dist_3_sqr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQFmmEQgxI0Y"
      },
      "source": [
        "dist_7_abs = (a_3 - mean7).abs().mean()\n",
        "dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\n",
        "dist_7_abs, dist_7_sqr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aeygTpnx77o"
      },
      "source": [
        "- The distance of our digit to the mean of three is closer than its distance to the mean of seven. This is true for both the `mean absolute error` and the `root mean squared error`.\n",
        "\n",
        "- This means that we can predict that our digit is a three.\n",
        "\n",
        "- In this case our simple model gives a correct prediction.\n",
        "\n",
        "- PyTorch already provides both the `MAE` & `RMSE` as loss functions inside the `torch.nn.functional` (imported as `F`).\n",
        "\n",
        "- In it *MSE* refers to `mean squared error` and *L1* refers to the `mean absolute error` (`L1 norm` in math)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aY_miXBxdel"
      },
      "source": [
        "F.l1_loss(a_3.float(), mean7), F.mse_loss(a_3.float(), mean7).sqrt()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBfocDeRZLSx"
      },
      "source": [
        "# Computing metrics using broadcasting\n",
        "\n",
        "- A metric tells us how good our model is by comparing the prediction to the correct label.\n",
        "\n",
        "- We use *accuracy* as a metric in classification models since it is understandable to most people"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX9-su1jYXQm"
      },
      "source": [
        "valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()])\n",
        "valid_3_tens = valid_3_tens.float()/255\n",
        "\n",
        "valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()])\n",
        "valid_7_tens = valid_7_tens.float()/255\n",
        "\n",
        "valid_3_tens.shape, valid_7_tens.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33D1fVxdbBTK"
      },
      "source": [
        "- We want to write a function `is_3` that will decide if an image is a 3 or a 7 depending on which it is closer to.\n",
        "\n",
        "- For this, we'll need to define a notion of distance, i.e. a function that calculates the distance between two images.\n",
        "\n",
        "- We'll do this using the mean absolute error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYloLHDCarg0"
      },
      "source": [
        "# mean((-1, -2)) -> take the mean along the last and the second last axis i.e the x and y axis\n",
        "def mnist_distance(a, b): return (a - b).abs().mean((-1, -2))\n",
        "mnist_distance(a_3, mean3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qh7aEfZdIvJ"
      },
      "source": [
        "- In order to calculate overall accuracy, we'll need to calculate the distance to the ideal three for every image in the validation set.\n",
        "\n",
        "- Our distance function is designed for comparing two single images but can work for our stacked tensor through `broadcasting`.\n",
        "\n",
        "- `Broadcasting` -> a PyTorch feature that automatically expands the tensor of the smaller rank to have the same size as the one with the larger rank.\n",
        "\n",
        "- In our case it treats `mean3` as 1010 copies of the same image and subtracts each of those copies from each \"three\" in `valid_3_tens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09K9jyhKbz-E"
      },
      "source": [
        "valid_3_distance = mnist_distance(valid_3_tens, mean3)\n",
        "valid_3_distance, valid_3_distance.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVEo4T9XetwK"
      },
      "source": [
        "def is_3(x): return mnist_distance(x, mean3) < mnist_distance(x, mean7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl24uySynwbs"
      },
      "source": [
        "is_3(a_3), is_3(a_3).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUpmsBM3n9ub"
      },
      "source": [
        "- Thanks to `broadcasting`, we can test it on the full validation set of threes.\n",
        "\n",
        "- This enables us to get rid of loops since loops make things slower moreso on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBRJK8Jpn6Yy"
      },
      "source": [
        "is_3(valid_3_tens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mjoL1sWorUc"
      },
      "source": [
        "accuracy_3s = is_3(valid_3_tens).float().mean()\n",
        "\n",
        "accuracy_7s = (1 - is_3(valid_7_tens).float()).mean()\n",
        "\n",
        "accuracy_3s, accuracy_7s, (accuracy_3s + accuracy_7s)/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lvnfUqspb_N"
      },
      "source": [
        "- As we can see, we have a model with a 95% accuracy in predicting 3s and 7s using broadcasting.\n",
        "\n",
        "- Our model however, cannot be defined as a machine learning model according to Arthur Samuel i.e *Automatic means of testing the actual effectiveness of any parameter assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximise the performance.*\n",
        "\n",
        "- To do better, let's try a system that does some real learning, i.e. can automatically modify itself to improve its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH1a990Kqx2s"
      },
      "source": [
        "# Stochastic Gradient Descent (SGD)\n",
        "\n",
        "- Instead of trying to find the similarity of an image with an \"ideal\" image, we could instead look at each individual pixel, and come up with a set of weights for each pixel, such that the highest weights are associated with those pixels most likely to be black for a particular category.\n",
        "\n",
        "- For instance, pixels towards the bottom right are not very likely to be activated for a seven so they should have low weight for a seven, but are more likely to be activated for an eight so they should have high weight for an eight.\n",
        "\n",
        "- We can represent this as a function:\n",
        "    ```python\n",
        "      def pr_eight(x,w) = (x*w).sum()\n",
        "    ```\n",
        "\n",
        "- Here we are assuming that `X` is an image represented as a vector and the weights are a vector `W`.\n",
        "\n",
        "- If the image we're looking at has pixels with a high weight it's going to end up with a high probability.\n",
        "\n",
        "- If we have this function, then we just need some way to update the weights to make them a little bit better.\n",
        "\n",
        "- With such an approach, we can repeat this step a number of times, making the weights better and better until they are as good as we can make them.\n",
        "\n",
        "- Searching for the best vector `W` is a way to search for the best function for recognising a digit.\n",
        "\n",
        "- To be more specific, below are the steps required to turn this function into a machine learning classifier:\n",
        "  - *Initialize* the weights. We initialize the weights to random values since we have a routine to improve these weights.\n",
        "  - For each image, use the weights to predict whether it appears to be a three or a seven.\n",
        "  - Based on these predictions, calculate how good the model is (its `loss`).\n",
        "  - Calculate the `gradient`, which measures for each weight, how changing that weight would change the loss.\n",
        "  - `Step` (change) all the weights based on the `gradient` calculation.\n",
        "  - Go back to step one and repeat the process.\n",
        "  - Stop training when you decide that the model is good enough, or you don't want to wait any longer.\n",
        "\n",
        "- These 7 steps are the key to the training of all deep learning models.\n",
        "\n",
        "- This technique is called `Gradient Descent`\n",
        "\n",
        "- Before applying these steps to our image classification problem, let's illustrate what they look like in a simpler case.\n",
        "\n",
        "- We'll define a simple function (the quadratic) and assume it's our loss function and x is our weighted parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUZJC31fpHG_"
      },
      "source": [
        "def f(x): return x**2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqKia6d_ZbYI"
      },
      "source": [
        "plot_function(f, 'x', 'x**2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zSQocWwc2Ek"
      },
      "source": [
        "- The sequence of steps we described above starts by picking some random value for a parameter, and calculating the value of the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C3g8hw5ZgKn"
      },
      "source": [
        "plot_function(f, 'x', 'x**2')\n",
        "plt.scatter(-1.5, f(-1.5), color='red');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkRo2hL_dTnK"
      },
      "source": [
        "- We now look to see what would happen if we increased or decreased our parameter by a little bit - the `adjustment`.\n",
        "\n",
        "- This is simply the slope at a particular point.\n",
        "\n",
        "- We can change our weight by a little in the direction of the slope, calculate our loss and adjustment again, and repeat this a few times.\n",
        "\n",
        "- Eventually, we'll get to the lowest point on the curve.\n",
        "\n",
        "- The basic idea goes back to Isaac Newton, who pointed out that we can optimize arbitrary functions this way.\n",
        "\n",
        "- Regardless of how complicated our functions become, this basic approach of gradient descent will not significantly change.\n",
        "\n",
        "- The only minor changes are some handy ways we can make it faster, by finding better steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lINXkIkbeuzB"
      },
      "source": [
        "## The gradient\n",
        "\n",
        "- The one magic step is the bit where we calculate the *gradient*.\n",
        "\n",
        "- We use calculus as a performance optimization; it allows us to more quickly calculate whether our loss will go up or down when we adjust our parameters up or down.\n",
        "\n",
        "- In other words, the gradients will tell us how much we have to change each weight to make our model better.\n",
        "\n",
        "- From highschool math, gradient is defined as \"rise/run\" i.e. the change in the value of the function divided by the change in the value of the parameter.\n",
        "\n",
        "- The *derivative* of a function tells you how much a change in the parameters of a function will change its result.\n",
        "\n",
        "- It is a function that calculates the change rather than the value.\n",
        "\n",
        "- The derivative of the quadratic function at the value three tells us how rapidly the function changes at the value three.\n",
        "\n",
        "- When we know how our function will change, then we know what to do to make it smaller.\n",
        "\n",
        "- This is the key to machine learning: having a way to change the parameters of a function to make it smaller.\n",
        "\n",
        "- Our function has lots of weights that we need to adjust, so when we calculate the derivative, we won't get back one number but lots of them - a gradient for every weight.\n",
        "\n",
        "- You can calculate the derivative with respect to one weight and treat all the others as constant, then repeat that for each weight.\n",
        "\n",
        "- This is how gradients are calculated for every weight.\n",
        "\n",
        "- Luckily PyTorch is able to automatically compute the derivative of any function so you don't have to calculate any gradients yourself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suQsrB9hdJSR"
      },
      "source": [
        "# Pick a tensor value which we want gradients at\n",
        "xt = tensor(3.).requires_grad_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO4LIUAf0YCJ"
      },
      "source": [
        "- `requires_grad_()` tells PyTorch that we want to calculate gradients with respect to that variable at that value.\n",
        "\n",
        "- An underscore at the end of a method in PyTorch means that it is an `in-place operation`.\n",
        "\n",
        "- An `in-place operation` directly changes the content of a given linear algebra, vector or tensor without making a copy.\n",
        "\n",
        "- In our function above `requires_grad_` modifies `tensor(3.)` to tell PyTorch that we want to calculate gradients on it.\n",
        "\n",
        "- We now calculate our function with that value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMjpu20r0W5P"
      },
      "source": [
        "yt = f(xt)\n",
        "yt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L11IKA2_1fzS"
      },
      "source": [
        "- Finally, we tell PyTorch to calculate the gradient for us"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaodCS0Z1b9s"
      },
      "source": [
        "yt.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuMhNYlb13pg"
      },
      "source": [
        "- `backward()` refers to `back propagation` which is the name given to the process of calculating the derivative of each layer.\n",
        "\n",
        "- We can now view the gradients by checking the `grad` attribute of our tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW_YsswI2LSn"
      },
      "source": [
        "xt.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUsIPryC1nJ0"
      },
      "source": [
        "xt = tensor([3.,4.,10.]).requires_grad_()\n",
        "xt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxhrSc6-2l7g"
      },
      "source": [
        "def f(x): return (x**2).sum()\n",
        "\n",
        "yt = f(xt)\n",
        "yt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI1878Nx2zM1"
      },
      "source": [
        "- Our gradients are 2*x as expected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUGDA3LY2tlO"
      },
      "source": [
        "yt.backward()\n",
        "xt.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ispiB1NH3E6Y"
      },
      "source": [
        "- The gradient only tells us the slope of our function, it doesn't actually tell us exactly how far to adjust the parameters.\n",
        "\n",
        "- It gives us some idea of how far, if the slope is very large it suggests that we have more adjustments to do.\n",
        "\n",
        "- Whereas if the slope is very small it may suggest that we are close to the optimal value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hauqyr1W2rkU"
      },
      "source": [
        "## Stepping with a learning rate\n",
        "\n",
        "- Deciding how to change our parameters based on the value of the gradients is an important part of the deep learning process.\n",
        "\n",
        "- Nearly all approaches start with the basic idea of multiplying the gradient by some small number called the `learning rate (LR)`.\n",
        "\n",
        "- People often select a learning rate just by trying a few numbers and finding which one results in the best model after training. The learning rate is often a number between 0.001 and 0.1, although it could be any number.\n",
        "\n",
        "- Once you've picked your learning rate, you can adjust your parameters using this function:\n",
        "  ```\n",
        "  w -= gradient(w) * lr\n",
        "  ```\n",
        "\n",
        "- This is known as `stepping` your parameters using an `optimizer step`.\n",
        "\n",
        "- If you pick a learning rate that's too low it can mean having to do a lot of steps.\n",
        "\n",
        "- Picking a learning rate that's too high is even worse, it can result in the loss getting worse or it may bounce around rather than actually diverging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-xLGznn47XU"
      },
      "source": [
        "## An end-to-end SGD example\n",
        "\n",
        "- Let's look at an SGD example and see how finding a minimum can be used to train a model to fit data better.\n",
        "\n",
        "- Imagine you were measuring the speed of a roller coaster as it went over the top of a hump. It would start fast, and then get slower as it went up the hill, and then would be slowest at the top, and it would speed up as it went down the hill. You want to build a model of how the space changes over time. If you're measuring the speed manually every second for 20 seconds, it might look something like this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9OIqFKk26V2"
      },
      "source": [
        "time = torch.arange(0,20).float(); time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH8Qk3LV5y42"
      },
      "source": [
        "# torch.randn(20)*3 adds a random number to every observation since measuring things manually isn't precise\n",
        "# The speed is calculated as the result of a quadratic function\n",
        "speed = torch.randn(20)*3 + 0.75*(time - 9.5)**2 + 1\n",
        "plt.scatter(time, speed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbZsuVJ9mefe"
      },
      "source": [
        "- ```torch.randn(20)*3``` adds a bit of random noise since measuring things manually isn't precise.\n",
        "\n",
        "- Using SGD we can try to find a function that matches our observations.\n",
        "\n",
        "- We can't consider every possible function, so let's use a guess that it'll be quadratic, i.e. a function of the form `a*(time**2) + (b*time) + c`.\n",
        "\n",
        "- We want to distinguish clearly between the functions input (the time we're measuring the coaster's speed) and its parameters (the values that define which quadratic we're trying).\n",
        "\n",
        "- Let's collect the parameters in one argument and thus separate the input `t` and the parameter `params` in the function's signature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBVe356Eluyg"
      },
      "source": [
        "def f(t, params):\n",
        "  a,b,c = params\n",
        "  return a*(t**2) + (b*t) + c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xO6NxxFnwR1"
      },
      "source": [
        "- We've restricted the problem of finding the best imaginable function that fits the data to finding the best quadratic function.\n",
        "\n",
        "- This greatly simplifies the problem, since every quadratic function is fully defined by the three parameters `a`, `b`, and `c`.\n",
        "\n",
        "- To find the best quadratic function, we need to find the best values for `a`, `b`, and `c`.\n",
        "\n",
        "- If we can solve the problem for the three parameters of the quadratic function, we'll be able to apply the same approach for other more complex functions with more parameters such as a neural net.\n",
        "\n",
        "- We need to define what we mean by \"best\". We define this precisely by choosing a *loss function*, which will return a value based on a prediction and a target, where lower values of the function correspond to \"better\" predictions.\n",
        "\n",
        "- For continuous data, it's common to use *mean squared error*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moj8lmf_l0B8"
      },
      "source": [
        "def mse(preds, targets): return ((preds - targets)**2).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twDAAQ_6iRpI"
      },
      "source": [
        "- Let's work through our 7 step process:\n",
        "\n",
        "**Step 1**:\n",
        "- Initialize parameters to random values and tell PyTorch that we want to track their gradient using `requires_grad_`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFhet4a9pZrt"
      },
      "source": [
        "params = torch.randn(3).requires_grad_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykkgLWlqisCD"
      },
      "source": [
        "orig_params = params.clone()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riD9Kkx2iwHo"
      },
      "source": [
        "**Step 2:**\n",
        "- Calculate the predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ah0Aan_iur2"
      },
      "source": [
        "preds = f(time, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1QWqESjjAko"
      },
      "source": [
        "- Let's create a little function to see how close our predictions are to our targets and take a look."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVcmMLNgi_7t"
      },
      "source": [
        "def show_preds(preds, ax=None):\n",
        "  if ax is None: ax = plt.subplots()[1]\n",
        "  ax.scatter(time, speed)\n",
        "  ax.scatter(time, to_np(preds), color='red')\n",
        "  ax.set_ylim(-300, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXn_oKi1jXuc"
      },
      "source": [
        "show_preds(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2j2nsH_jgbS"
      },
      "source": [
        "- This doesn't look very close -- our random parameters suggest that the roller coaster will end up going backwards since we have negative speeds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4qeVLB3jsbl"
      },
      "source": [
        "**Step 3:**\n",
        "- Calculate the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHiQ309Rjc-8"
      },
      "source": [
        "loss = mse(preds, speed)\n",
        "loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7S_QusHkHAL"
      },
      "source": [
        "- Our goal is to improve this. To do that we'll need the gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u_oLkbJkNJG"
      },
      "source": [
        "**Step 4:**\n",
        "- Calculate the gradients (an approximation of how the parameters need to change)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KRfd2eVj5_i"
      },
      "source": [
        "loss.backward()\n",
        "params.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIR8qxnpkkgY"
      },
      "source": [
        "- We pick a learning rate of $10^{-5}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8OU7LB7kitF"
      },
      "source": [
        "params.grad * 1e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voo4ddlDk0wV"
      },
      "source": [
        "- We can use these gradients to improve our parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HoSFQG9kzaQ"
      },
      "source": [
        "params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxLHOu13lC6d"
      },
      "source": [
        "**Step 5:**\n",
        "- Step the weights (update the parameters based on the gradients we just calculated)\n",
        "\n",
        "- We use the `.data` attribute in PyTorch to make sure that the gradient isn't calculated of the actual step we're doing.\n",
        "\n",
        "- The gradient should only be calculate of the function `f`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5JOzd4Dk-6Z"
      },
      "source": [
        "lr = 1e-5\n",
        "params.data -= lr * params.grad.data\n",
        "params.grad = None  # remove the gradients previously calculated."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlFGbq3FmOeA"
      },
      "source": [
        "- Let's see if the loss improved"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofgmUqC5mNpR"
      },
      "source": [
        "preds = f(time, params)\n",
        "mse(preds, speed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SYgu8aCmWdu"
      },
      "source": [
        "- We notice that the loss has indeed reduced from the initial calculation.\n",
        "\n",
        "- We also plot the function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P_3APoxmVNk"
      },
      "source": [
        "show_preds(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohq8LRzSmnBO"
      },
      "source": [
        "- The plot has also improved.\n",
        "\n",
        "- We need to repeat this a few times, so we'll create a function to apply one step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwECke1omhQS"
      },
      "source": [
        "def apply_step(params, prn=True):\n",
        "  preds = f(time, params)\n",
        "  loss = mse(preds, speed)\n",
        "  loss.backward()\n",
        "  params.data -= lr * params.grad.data\n",
        "  params.grad = None\n",
        "  if prn: print(loss.item())\n",
        "  return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi9hI9jgnFee"
      },
      "source": [
        "**Step 6:**\n",
        "- We repeat the process.\n",
        "\n",
        "- By looping and performing many improvements, we hope to reach a good result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwF1Vbw8nFHB"
      },
      "source": [
        "for i in range(10): apply_step(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOfsq7Y_nTO4"
      },
      "source": [
        "params = orig_params.detach().requires_grad_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hpto2dEnbOa"
      },
      "source": [
        "- The loss is going down just as we'd hoped.\n",
        "\n",
        "- Looking only at the loss disguises the fact that each iteration represents an entirely different quadratic function being tried on the way to find the best possible quadratic function.\n",
        "\n",
        "- We can see this process visually if, instead of printing out the loss, we plot the function at every step.\n",
        "\n",
        "- Then we can see how the shape is approaching the best possible quadratic function for our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz1061JpnazX"
      },
      "source": [
        "_, axs = plt.subplots(1, 4, figsize=(12,3))\n",
        "for ax in axs: show_preds(apply_step(params, False), ax)\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4CwUzWZoazB"
      },
      "source": [
        "**Step 7:**\n",
        "\n",
        "- Stop.\n",
        "\n",
        "- We decided to stop arbitrarily after 10 epochs.\n",
        "\n",
        "- In practice, we watch the training and validation losses and our metrics decide when to stop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvyTyyyLosaQ"
      },
      "source": [
        "# MNIST loss function\n",
        "\n",
        "- We need gradients in order to improve our model using SGD, and in order to calculate gradients, we need some loss function that represents how good our model is.\n",
        "\n",
        "- Gradients are a measure of how that loss function changes with small tweaks to the weights.\n",
        "\n",
        "- In choosing a loss function, the obvious approach would be to use accuracy, which is our metric, as our loss function as well.\n",
        "\n",
        "- In this case, we would calculate our prediction for each image, collect these values to calculate an overall accuracy, and then calculate the gradients of each weight with respect to the overall accuracy.\n",
        "\n",
        "- The gradient of a function is how much the value of a function goes up or down divided by how much you changed the inputs.\n",
        "\n",
        "- Accuracy only changes when a prediction changes from a 3 to a 7 or vice versa meaning that a small change in weights isn't likely to cause any prediction change.\n",
        "\n",
        "- Therefore the problem with accuracy is that the gradient will be zero almost everywhere because a very small change in the value of a weight will often not change the accuracy at all.\n",
        "\n",
        "- This makes accuracy a poor choice as a loss function because with the gradients as zero the model is unable to learn from that number.\n",
        "\n",
        "- We need a loss function which, when our weights result in slightly better predictions, gives us a slightly better loss.\n",
        "\n",
        "- We already have our `x`'s i.e. the images themselves.\n",
        "\n",
        "- We'll concatenate them into a single tensor using `torch.cat()`, and also change them from a list of matrices (rank 3 tensor) to a list of vectors (rank 2 tensor) using the `view` method.\n",
        "\n",
        "- `view` is a PyTorch method that changes the shape of a tensor without changing its contents.\n",
        "\n",
        "- `-1` is a special parameter to `view` that means make this axis as big as necessary to fit all the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV2pYecuoV7V"
      },
      "source": [
        "# view(-1, 28*28) -> put everything in as many rows of length 28*28 as is necessary\n",
        "train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DzENEn27GzQ"
      },
      "source": [
        "- We need a label for each. We'll use 1 for threes and 0 for sevens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y88ed6os7GQr"
      },
      "source": [
        "# unsqueeze(1) create a matrix with one column\n",
        "train_y = tensor([1] * len(threes) + [0] * len(sevens)).unsqueeze(1)\n",
        "train_x.shape, train_y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrqEFe_z7g-z"
      },
      "source": [
        "- A dataset in PyTorch is required to return a tuple of (x, y) when indexed.\n",
        "\n",
        "- Python provides a `zip` function which when combined with a list, provides a simple way to get this functionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGdQVWNA7cov"
      },
      "source": [
        "dset = list(zip(train_x, train_y))\n",
        "x,y = dset[0]  # destructuring the tuple\n",
        "x.shape, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORzCY1Xk75eC"
      },
      "source": [
        "valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\n",
        "valid_y = tensor([1] * len(valid_3_tens) + [0] * len(valid_7_tens)).unsqueeze(1)\n",
        "valid_dset = list(zip(valid_x, valid_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIviZEgxwbu0"
      },
      "source": [
        "- We initialize a random weight for every pixel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOO7z9H1_OXz"
      },
      "source": [
        "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4jJR1TTwtBj"
      },
      "source": [
        "weights = init_params((28*28, 1))  # the 1 tells Pytorch that we want the results as a column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtTgJMSMw_2W"
      },
      "source": [
        "- The function `weights*pixels` won't be flexible enough.\n",
        "\n",
        "- It's always equal to zero when the pixels are equal to zero i.e. its intercept is zero.\n",
        "\n",
        "- The formula for a line is `y=w*x_b`; we still need the b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjhjGU_bwzBa"
      },
      "source": [
        "bias = init_params(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZzR_CzCxZd5"
      },
      "source": [
        "bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lcjKJKnyrJr"
      },
      "source": [
        "- In neural networks, the `w` in the equation `y=w*x+b` is called the *weights* and `b` is called the *bias*.\n",
        "\n",
        "- Together, the weights and bias make up the parameters.\n",
        "\n",
        "- We can now calculate the prediction for one image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y906iKDIxaPX"
      },
      "source": [
        "(train_x[0]*weights.T).sum() + bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m36OafDr0YQ0"
      },
      "source": [
        "- We don't use a python for loop to calculate the prediction of each image.\n",
        "\n",
        "- This is because for loops are very slow and don't run on the GPU.\n",
        "\n",
        "- We need to represent as much of the computation in a model as possible using higher-level functions\n",
        "\n",
        "- We can use *matrix multiplication* to calculate `w*x` for every row of a matrix.\n",
        "\n",
        "- In Python, matrix multiplication is represented with the `@` operator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu_dKH5Fzm1Y"
      },
      "source": [
        "def linear1(xb): return xb@weights + bias\n",
        "preds = linear1(train_x)\n",
        "preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSvhG8LFr1yd"
      },
      "source": [
        "- The `batch@weights + bias` equation is one of the two fundamental equations of any neural network (the other one is the *activation function*).\n",
        "\n",
        "- To decide if an output represents a 3 or a 7, we can just check if its greater than zero.\n",
        "\n",
        "- Our accuracy for each item can be calculated with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy7vwCOerj7i"
      },
      "source": [
        "corrects = (preds>0.0).float() == train_y  # broadcasting\n",
        "corrects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpO2O8gNsfUs"
      },
      "source": [
        "# .item() unwraps a tensor to create a normal python scalar\n",
        "corrects.float().mean().item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loFl_OkptTAs"
      },
      "source": [
        "- Our randomly initialized model is right about half the time at predicting 3's from 7's\n",
        "\n",
        "- Let's see what the change in accuracy is for a small change in one of the weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tiAIctqtJac"
      },
      "source": [
        "weights[0] *= 1.0001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCsR-i7ttz4M"
      },
      "source": [
        "preds = linear1(train_x)\n",
        "((preds>0.0).float() == train_y).float().mean().item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7ZkXLF2uJSw"
      },
      "source": [
        "- We get back the exact same figure.\n",
        "\n",
        "- We can see that we need gradients in order to improve our model using SGD, and in order to calculate gradients we need some *loss function* that represents how good our model is.\n",
        "\n",
        "- Gradients are a measure of how that loss function changes with small tweaks to the weights.\n",
        "\n",
        "- Accuracy as a loss function will not be useful in this case, due to the gradient being zero most of the time.\n",
        "\n",
        "- We need a loss function which, when our weights result in slightly better predictions, gives us a better loss.\n",
        "\n",
        "- The loss function receives not the images themselves, but the prediction from the model.\n",
        "\n",
        "- One of the arguments to the loss function will be `predictions`, a vector (rank-1 tensor), indexed over the images, of values between 0 and 1, where each value is the prediction indicating how likely the component's image is a 3.\n",
        "\n",
        "- The purpose of the loss function is to measure the difference between predicted values and true values (labels/targets).\n",
        "\n",
        "- The loss will be smaller i.e. better when the predictions are closer to the targets.\n",
        "\n",
        "- Another argument to the loss function will be the `targets`, a vector indexed over the images, with a value of 0 or 1 that tells us whether the image is actually a 3.\n",
        "\n",
        "- For example, let's say we had three images of a 3, a 7 and a 3. Suppose our model predicted with high confidence the first was a 3, slight confidence the second was a 7 and fair confidence (incorrectly) that the last was a 7. This would mean our loss function would receive these values as inputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtH9N9NyuB9u"
      },
      "source": [
        "trgts = tensor([1,0,1])\n",
        "prds = tensor([0.9, 0.4, 0.2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF5EokKVxh9v"
      },
      "source": [
        "- Here's a first try at a loss function that measures the distance between predictions and targets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BKMGRAYxhFR"
      },
      "source": [
        "def mnist_loss(predictions, targets):\n",
        "  \"\"\"Return 1-predictions where targets==1 and predictions where targets==0\"\"\"\n",
        "  return torch.where(targets==1, 1-predictions, predictions).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wqPhw2jyEqh"
      },
      "source": [
        "- `torch.where(a, b, c)` is the equivalent of running the list comprehension `[b[i] if a[i] else c[i] for i in range(len(a))], except it works on tensors, at C/CUDA speeds.\n",
        "\n",
        "- Our function measures how distant each prediction is from 1 if it should be 1, and how distant it is from 0 if it should be 0, and then takes the mean of all those distances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9aeosINyDzt"
      },
      "source": [
        "torch.where(trgts==1, 1-prds, prds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xMwuB4xyj4Z"
      },
      "source": [
        "mnist_loss(prds, trgts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrRdAml9zjB4"
      },
      "source": [
        "- If we change the prediction for the one \"false\" target from 0.2 to 0.8, the loss will go down indicating that this is a better prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPYmcnALzTk1"
      },
      "source": [
        "mnist_loss(tensor([0.9, 0.4, 0.8]), trgts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuGj3uM60Gsb"
      },
      "source": [
        "- `mnist_loss` as it is currently defined assumes that predictions are always between zero and one.\n",
        "\n",
        "- We need to ensure that this is always the case.\n",
        "\n",
        "- The `sigmoid function` always outputs a number between zero and one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTrG4alQ0YBn"
      },
      "source": [
        "## Sigmoid\n",
        "\n",
        "- The `sigmoid` function is defined by:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqYdwJcg0pLr"
      },
      "source": [
        "# torch.exp is the same as e\n",
        "def sigmoid(x): return 1/(1+torch.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTJjXJ9c1kX1"
      },
      "source": [
        "math.e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DztAd-pj1oq1"
      },
      "source": [
        "math.e**2 == torch.exp(tensor(2.))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8EfE0J400Lj"
      },
      "source": [
        "- PyTorch already defines this for us.\n",
        "\n",
        "- The sigmoid function is important in deep learning since we always want to ensure values are between zero and one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtE0Fn6iz3Ih"
      },
      "source": [
        "plot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsBaI4iw2Rph"
      },
      "source": [
        "- The sigmoid function takes in any input value, positive or negative and outputs a value between zero and one.\n",
        "\n",
        "- Smaller numbers give an output closer to zero and larger numbers give an output closer to one.\n",
        "\n",
        "- It never goes below zero and beyond one.\n",
        "\n",
        "- It's also a smooth curve that only goes up, making it easier for SGD to find meaningful gradients.\n",
        "\n",
        "- We update `mnist_loss` to apply `sigmoid` to the inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3QlNXWX1HxZ"
      },
      "source": [
        "def mnist_loss(predictions, targets):\n",
        "  predictions = predictions.sigmoid()\n",
        "  return torch.where(targets==1, 1-predictions, predictions).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJecIUIP8BcS"
      },
      "source": [
        "- We can now be confident that our loss function will work, even when the predictions are not between 0 and 1.\n",
        "\n",
        "- Metrics, are the numbers we really care about i.e. accuracy.\n",
        "\n",
        "- These are things printed out at the end of every epoch that tell us how our model is really doing.\n",
        "\n",
        "- When judging the performance of a model it is important to focus on the metrics.\n",
        "\n",
        "- The loss function needs to respond to small changes in confidence level.\n",
        "\n",
        "- This means that sometimes the loss doesn't really reflect what we are trying to achieve, but is rather a compromise between our real goal, and a function that can be optimized using its gradient.\n",
        "\n",
        "- The loss function is calculated for each item in our dataset, and at the end of an epoch these are all averaged, and the overall mean is reported for the epoch.\n",
        "\n",
        "- In summary, the key difference between the loss and the metric is that the metric is to drive human understanding and the loss is to drive automated learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEuxhhnF_0ZV"
      },
      "source": [
        "### SGD and mini-batches\n",
        "\n",
        "- Now that we have a loss function that is suitable to drive SGD, we can now consider the `optimization step`.\n",
        "\n",
        "- The optimization step is where we step (change or update) the weights based on the gradient.\n",
        "\n",
        "- To do this we need to calculate the loss of one or more data items.\n",
        "\n",
        "- We calculate the loss of a few data items at a time, called a `mini-batch`.\n",
        "\n",
        "- The number of items in a mini-batch is called a `batch size`.\n",
        "\n",
        "- We take mini-batches because:\n",
        "  - Calculating the loss for the whole dataset takes a very long time.\n",
        "  - Calculating the loss for each individual item does not use much information and results in a very imprecise and unstable gradient.\n",
        "\n",
        "- A larger batch size means that you will get a more accurate and stable estimate of your dataset's gradient on the loss function, but it takes longer and you will get less mini-batches per epoch.\n",
        "\n",
        "- Mini-batches are ideal since we train on a GPU and GPUs only perform well if they have lots of work to do at a time.\n",
        "\n",
        "- From data augmentation, we get better generalization if we vary things during training.\n",
        "\n",
        "- A simple and effective thing we can do is to randomly shuffle our dataset on every epoch before we create mini-batches.\n",
        "\n",
        "- PyTorch and fastai provide a `DataLoader` that does the shuffling and mini batch collation for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WQBp40X7XJ6"
      },
      "source": [
        "coll = range(15)\n",
        "dl = DataLoader(coll, batch_size=5, shuffle=True)\n",
        "list(dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3E9DGbLqNQv"
      },
      "source": [
        "- `Dataset` - A PyTorch collection that contains tuples of independent and dependent variables\n",
        "\n",
        "- For training a model, we just don't want any collection but a PyTorch collection i.e. a *dataset*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1p5yr2uDBA-"
      },
      "source": [
        "ds = L(enumerate(string.ascii_lowercase))\n",
        "ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT55w85Sqv3o"
      },
      "source": [
        "- When we pass a Dataset to a Dataloader, we get back many batches that are themselves tuples of tensors representing batches of independent and dependent variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RSC5Yb_qoin"
      },
      "source": [
        "dl = DataLoader(ds, batch_size=6, shuffle=True)\n",
        "list(dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB_EQzE9rsEQ"
      },
      "source": [
        "### Putting it all together\n",
        "\n",
        "- We can now write our first training loop for a model using SGD.\n",
        "\n",
        "- In code, our process will be implemented something like this for every epoch:\n",
        "```python\n",
        "    for x,y in dl:\n",
        "        pred = model(x)\n",
        "        loss = loss_func(pred, y)\n",
        "        loss.backward()\n",
        "        parameter -= parameter.grad - lr\n",
        "```\n",
        "\n",
        "- First, let's re-initialize our parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFq24a_WrAMp"
      },
      "source": [
        "weights = init_params((28*28,1))\n",
        "bias = init_params(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CGi-2b3tTiD"
      },
      "source": [
        "- A `DataLoader` can be created from a `Dataset`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVDM58LIs2ww"
      },
      "source": [
        "dl = DataLoader(dset, batch_size=256)\n",
        "xb, yb = first(dl)\n",
        "xb.shape, yb.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdlESscWtqQh"
      },
      "source": [
        "- We'll do the same for the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb12G71VthMQ"
      },
      "source": [
        "valid_dl = DataLoader(valid_dset, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnI5HuHpt0_x"
      },
      "source": [
        "- Let's create a mini-batch of size 4 for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpVLXVG4t0LA"
      },
      "source": [
        "batch = train_x[:4]\n",
        "batch.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAs6mE8V8X_j"
      },
      "source": [
        "preds = linear1(batch)\n",
        "preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb0aveCh8jdD"
      },
      "source": [
        "loss = mnist_loss(preds, train_y[:4])\n",
        "loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWc52hFK9Aas"
      },
      "source": [
        "- Now we calculate the gradients:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMq0lSXg8zm1"
      },
      "source": [
        "loss.backward()\n",
        "weights.grad.shape, weights.grad.mean(), bias.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8O4e2Br9rWg"
      },
      "source": [
        "- Let's put all that in a function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZxf6BT69LWz"
      },
      "source": [
        "def calc_grad(xb, yb, model):\n",
        "  preds = model(xb)\n",
        "  loss = mnist_loss(preds, yb)\n",
        "  loss.backward()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scKAiR1T-GyS"
      },
      "source": [
        "calc_grad(batch, train_y[:4], linear1)\n",
        "weights.grad.mean(), bias.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rx83ez1-RB0"
      },
      "source": [
        "calc_grad(batch, train_y[:4], linear1)\n",
        "weights.grad.mean(), bias.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxSl8xMkAHKS"
      },
      "source": [
        "- When we call the function a second time, notice that the gradients have changed. This is very concerning.\n",
        "\n",
        "- `loss.backward` not only calculates the gradient, it also adds the gradients of `loss` to any gradients that have been stored.\n",
        "\n",
        "- To prevent this from happening we have to set the current gradients to zero first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTNgTtTVAFsQ"
      },
      "source": [
        "weights.grad[541]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCIPQWa1AxZq"
      },
      "source": [
        "# change the weights.grad tensor to be a tensor containing zeros\n",
        "# the _ just means do it in place\n",
        "weights.grad.zero_()\n",
        "bias.grad.zero_();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_6BjfdFBng9"
      },
      "source": [
        "- The remaining step will be to update the weights and bias based on the gradient and learning rate.\n",
        "\n",
        "- We have to be careful and tell PyTorch not to take the gradient of this step too, otherwise it gets very confusing when we try to compute the derivative at the next batch.\n",
        "\n",
        "- We do this by assigning to the `data` attribute of a tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJgU_fSSBIeS"
      },
      "source": [
        "def train_epoch(model, lr, params):\n",
        "  \"\"\"A basic standard SGD loop.\"\"\"\n",
        "  for xb, yb in dl:\n",
        "    calc_grad(xb, yb, model)\n",
        "    for p in params:\n",
        "      p.data -= p.grad*lr\n",
        "      p.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lh0SOMkDXWF"
      },
      "source": [
        "> The difference between `gradient descent` and `stochastic gradient descent` is that gradient descent calculates the gradient on the entire dataset while stochastic gradient descent loops through a mini-batch and calculates the gradient on that"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWusz-IrCZhe"
      },
      "source": [
        "- We also want to know how we're doing by looking at the accuracy of the validation set.\n",
        "\n",
        "- To decide if an input represents a 3 or a 7, we can just check whether it's greater than zero.\n",
        "\n",
        "- The accuracy of each item can be calculated (using broadcasting) with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7JIoZRyCYz4"
      },
      "source": [
        "(preds>0.0).float() == train_y[:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOAs_vwkES8N"
      },
      "source": [
        "- The above gives us this function to calculate our validation accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6x1z59BC_nG"
      },
      "source": [
        "def batch_accuracy(xb, yb):\n",
        "  preds = xb.sigmoid()\n",
        "  correct = (preds>0.5) == yb\n",
        "  return correct.float().mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PARmR4rjFFtB"
      },
      "source": [
        "- You'll notice that we're working with the sigmoid of the predictions.\n",
        "\n",
        "- To measure the accuracy we use `(preds>0.5)`, this is because on the sigmoid curve 0 corresponds to 0.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjQm7-MUEj5z"
      },
      "source": [
        "batch_accuracy(linear1(batch), train_y[:4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjfs1dYHEsN1"
      },
      "source": [
        "def validate_epoch(model):\n",
        "  accs = [batch_accuracy(model(xb), yb) for xb, yb in valid_dl]\n",
        "  return round(torch.stack(accs).mean().item(), 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezSgLXn1GhpQ"
      },
      "source": [
        "validate_epoch(linear1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjmvJxUaNZB2"
      },
      "source": [
        "- Let's train for one epoch and see if the accuracy improves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x61Kodz-GkK1"
      },
      "source": [
        "lr = 1.\n",
        "params = weights,bias\n",
        "train_epoch(linear1, lr, params)\n",
        "validate_epoch(linear1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B_TovaLNx8d"
      },
      "source": [
        "for i in range(20):\n",
        "  train_epoch(linear1, lr, params)\n",
        "  print(validate_epoch(linear1), end=' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0VVe7bIOy2x"
      },
      "source": [
        "- We're already at about the same accuracy as our \"pixel similarity\" approach, and we've created a general purpose foundation we can build on.\n",
        "\n",
        "- The next step will be to create an object that will handle the SGD step for us. In PyTorch, it's called an *optimizer*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gkn1uf7QPKFE"
      },
      "source": [
        "## Creating an optimizer\n",
        "\n",
        "- PyTorch provides some useful classes to make implementation easier.\n",
        "\n",
        "- A \"module\" is an object of a class that inherits from the PyTorch `nn.module` class.\n",
        "\n",
        "- Objects of this class behave like a standart Python function, in that you can call it using parentheses, and it will return activations of a model.\n",
        "\n",
        "- First, we'll replace our `linear1()` function with PyTorch's `nn.Linear` module.\n",
        "\n",
        "- `nn.Linear` does the same thing as our `init_params` and `linear1` together.\n",
        "\n",
        "- It contains both the weights and bias in a single class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R65DIO8KODXf"
      },
      "source": [
        "linear_model = nn.Linear(28*28, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zloV2_kRNnO"
      },
      "source": [
        "- Every PyTorch module knows what parameters it has that can be trained; they are available through the `parameters` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTuWolT_RLXf"
      },
      "source": [
        "w,b = linear_model.parameters()\n",
        "w.shape, b.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsMyLG-YRgou"
      },
      "source": [
        "- We can use this information to create an optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY--ykf0RaLV"
      },
      "source": [
        "class BasicOptim:\n",
        "  def __init__(self, params, lr): self.params, self.lr = list(params), lr\n",
        "\n",
        "  def step(self, *args, **kwargs):\n",
        "    for p in self.params: p.data -= p.grad.data * self.lr\n",
        "\n",
        "  def zero_grad(self, *args, **kwargs):\n",
        "    for p in self.params: p.grad = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFb3GuEkSElf"
      },
      "source": [
        "- We can create our optimizer by passing in the model's parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smXw3Cs7SB6c"
      },
      "source": [
        "opt = BasicOptim(linear_model.parameters(),lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3IdpqvfSUM5"
      },
      "source": [
        "- Our training loop can now be simplified to:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6md-hzGSP3I"
      },
      "source": [
        "def train_epoch(model):\n",
        "  for xb, yb in dl:\n",
        "    calc_grad(xb, yb, model)\n",
        "    opt.step()\n",
        "    opt.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79kbLs3CSjFH"
      },
      "source": [
        "- Our validation function doesn't need to change"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpXtunU-Siqe"
      },
      "source": [
        "validate_epoch(linear_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyB-zD03SuBu"
      },
      "source": [
        "- Putting our training loop into a function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxJb3LmDSoQ-"
      },
      "source": [
        "def train_model(model, epochs):\n",
        "  for i in range(epochs):\n",
        "    train_epoch(model)\n",
        "    print(validate_epoch(model), end=' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jj30Ug0S69G"
      },
      "source": [
        "train_model(linear_model, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3giF44huTMVr"
      },
      "source": [
        "- fastai provides the `SGD` class which, by default, does the same thing as our `BasicOptim`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WXVcHTeS-BD"
      },
      "source": [
        "linear_model = nn.Linear(28*28, 1)\n",
        "opt = SGD(linear_model.parameters(), lr)\n",
        "train_model(linear_model, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYvm8ZT1ePLC"
      },
      "source": [
        "- fastai also provides a `Learner.fit` that can be used instead of `train_model`.\n",
        "\n",
        "- To create a `Learner` we first need to create `DataLoaders` by passing in our training and validation `DataLoader`.\n",
        "\n",
        "- `DataLoaders` stores away a `DataLoader` as a `.train` and a `.valid`.\n",
        "\n",
        "- With `DataLoaders` we now have a sigle object that knows all the data we have.\n",
        "\n",
        "- It can make sure that the training `DataLoader` is shuffled and the validation `DataLoader` isn't shuffled so that everything works properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXuP8CSwTj5I"
      },
      "source": [
        "dls = DataLoaders(dl, valid_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcbJS8aOfUZH"
      },
      "source": [
        "- To create a `Learner` class without using an application (such as `cnn_learner`), we need to pass in: the `DataLoaders`, the model, the optimization function, the loss function, and optionally any metrics to print"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj4fu7v2fTXr"
      },
      "source": [
        "learn = Learner(dls, nn.Linear(28*28, 1), opt_func=SGD,\n",
        "                loss_func=mnist_loss, metrics=batch_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEYsIWBhf9ew"
      },
      "source": [
        "- We can now call `fit`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWWJvcOJfz5-"
      },
      "source": [
        "learn.fit(10, lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3FVLa6XgYWg"
      },
      "source": [
        "- With these classes, we can now replace a linear function with a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApAaQpghh1wR"
      },
      "source": [
        "## Adding a non-linearity\n",
        "\n",
        "- A linear classifier is very constrained in terms of what it can do.\n",
        "\n",
        "- To make it a bit more complex (and able to handle more tasks), we need to add a non-linearity between two linear classifiers, and this is what gives us a neural network.\n",
        "\n",
        "- Below is the entire definition of a basic neural network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMaEIRqYgC7_"
      },
      "source": [
        "def simple_net(xb):\n",
        "  \"\"\"res.max(tensor(0.0)) takes any negative numbers and turns them into zeros.\n",
        "  The first and third lines are known as linear layers.\n",
        "  The second line is known as a nonlinearity or activation function.\n",
        "  \"\"\"\n",
        "  res = xb@w1 + b1\n",
        "  res = res.max(tensor(0.0))  # recified linear unit (ReLU)\n",
        "  res = res@w2 + b2\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M1O8ZQHfSaF"
      },
      "source": [
        "- Here, `w1` and `w2` are weight tensors and `b1` and `b2` are bias tensors; i.e. parameters that are initially randomly initialized.\n",
        "\n",
        "- Our `simple_net` function is an example of *function composition*.\n",
        "\n",
        "- `Function composition` - take the result of a function and pass it to a new one.\n",
        "\n",
        "- Every neural network is basically doing function composition of linear layers and activation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxBUAHWSi3Yo"
      },
      "source": [
        "w1 = init_params((28*28,30))\n",
        "b1 = init_params(30)\n",
        "w2 = init_params((30,1))\n",
        "b2 = init_params(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEV1NIZjf_WU"
      },
      "source": [
        "- `w1` has 30 output activations (meaning that `w2` must have 30 input activations so that they match).\n",
        "\n",
        "- This means that the first layer can construct 30 different features, each representing some different mix of pixels.\n",
        "\n",
        "- You can change the `30` to anything you like, to make the model more or less complex.\n",
        "\n",
        "- The function `res.max(tensor(0.0))` is called a *rectified linear unit (ReLU)*.\n",
        "\n",
        "- It replaces every negative number with zero.\n",
        "\n",
        "- It is also available in PyTorch as `F.relu`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwjVJbVqfyBh"
      },
      "source": [
        "plot_function(F.relu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVYDhkeEhrBZ"
      },
      "source": [
        "- According to the `Universal Approximation Theorem` if the size of the weights and bias matrices are big enough and you can find the right parameters, then our `simple_net` function can solve any computable problem to an arbitrarily high level of accuracy (including the function of how to recognize 3s and 7s.\n",
        "\n",
        "- This is only possible because of the `max` function that adds a non-linearity to our `simple_net` function.\n",
        "\n",
        "- Mathematically, the composition of two linear functions is a linear function. Without non-linear functions, we can stack as many linear classifiers on top of each other and it will just be the same as one linear classifier.\n",
        "\n",
        ">> A series of any number of linear layers in a row can be replaced with a single linear layer with a different set of parameters.\n",
        "\n",
        "- We can replace our `simple_net` code with something a bit simpler by taking advantage of PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzIkahINgvYA"
      },
      "source": [
        "simple_net = nn.Sequential(\n",
        "    nn.Linear(28*28, 30),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(30,1)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7hWY7p6xIKy"
      },
      "source": [
        "- `nn.Sequential` creates a module which will call each of the listed layers or functions in turn. It is an easier way to do function composition.\n",
        "\n",
        "- `F.relu` is a function, not a PyTorch module.\n",
        "\n",
        "- `nn.ReLU` is a PyTorch module that does exactly the same thing.\n",
        "\n",
        "- When using `nn.Sequential` PyTorch requires us to use the module version and since modules are classes, we have to instantiate them i.e. `nn.ReLU()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KaJNU6GxGOE"
      },
      "source": [
        "learn = Learner(dls, simple_net, opt_func=SGD,\n",
        "                loss_func=mnist_loss, metrics=batch_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlXF6vwZzv4r"
      },
      "source": [
        "learn.fit(40, 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fVl71LUz73s"
      },
      "source": [
        "> For deeper models, you may want to use a lower learning rate and a few more epochs.\n",
        "\n",
        "- The training process is recorded in `learn.recorder`, with the table of output stored in the `values` attribute.\n",
        "\n",
        "- We can plot the accuracy over training as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAS_X7aLz5KS"
      },
      "source": [
        "# itemgot(2) get the second item from every row (batch_accuracy) for plotting\n",
        "plt.plot(L(learn.recorder.values).itemgot(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN0j8Ryq0-RN"
      },
      "source": [
        "- ...and view the final accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHlXl4QE0foA"
      },
      "source": [
        "learn.recorder.values[-1][2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHDt5p0i1MXm"
      },
      "source": [
        "- At this point we have:\n",
        "  - A function that can solve any problem to any level of accuracy (the neural network) given the correct set of parameters.\n",
        "  - A way to find the best set of parameters for any function (stochastic gradient descent).\n",
        "\n",
        "- Below we see a way to look into what the model is doing.\n",
        "\n",
        "- To understand the features that make up the first layer, we can plot them as seen below. For layers other than the first though we have to be a lot more sophisticated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fEYCFrc1ErY"
      },
      "source": [
        "m = learn.model\n",
        "m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfMOQ_kIak0Y"
      },
      "source": [
        "m[0].parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1ifx4EBaoiP"
      },
      "source": [
        "w,b = m[0].parameters()\n",
        "w.shape, b.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9TB33Arasih"
      },
      "source": [
        "show_image(w[0].view(28,28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK55MbUUcXWz"
      },
      "source": [
        "- In practice, we can add as many linear layers as we want as long as we add a nonlinearity between each pair of linear layers (making the model deeper).\n",
        "\n",
        "- However, the deeper the model gets, the harder it is to optimise the parameters in practice.\n",
        "\n",
        "- **Why use deeper models?**\n",
        "> Performance. With a deeper model, we do not need to use as many parameters; we can use smaller matrices, with more layers, and get better results than we would with larger matrices and fewer layers. We can therefore train quicker and our model takes up less memory.\n",
        "\n",
        "- Below we see what happens when we train an 18 layer model using the same approach:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Xh1IM0gCbDu7",
        "outputId": "3b1bbe27-8f21-4ecf-98bf-c22a9dd8fd42"
      },
      "source": [
        "dls = ImageDataLoaders.from_folder(path)\n",
        "learn = cnn_learner(dls, resnet18, pretrained=False,\n",
        "                    loss_func=F.cross_entropy, metrics=accuracy)\n",
        "learn.fit_one_cycle(1, 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.060807</td>\n",
              "      <td>0.011237</td>\n",
              "      <td>0.998037</td>\n",
              "      <td>02:33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyWzqOydfEmF"
      },
      "source": [
        "- It gives nearly 100% accuracy, a big difference compared to our `simple_net` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynNtKk2Vggrq"
      },
      "source": [
        "### Summary of key concepts related to SGD.\n",
        "\n",
        "> Activations - numbers that are calculated.\n",
        "\n",
        "> Parameters - numbers that are randomly initialized, and optimised (the numbers we're learning).\n",
        "\n",
        "> ReLU - A function that returns zero for negative numbers and doesn't change for positive numbers.\n",
        "\n",
        "> Mini-batch - A small batch of inputs and labels gathered together in two arrays. A gradient descent step is updated on this batch (rather than a whole epoch).\n",
        "\n",
        ">Forward pass - Applying the model to some input and computing the predictions.\n",
        "\n",
        "> Loss - A value that represents how well (or badly) our model is doing.\n",
        "\n",
        "> Gradient - The derivative of the loss with respect  to some parameter of the model.\n",
        "\n",
        "> Backward pass - Computing the gradients of the loss with respect to all model parameters.\n",
        "\n",
        "> Gradient descent - Taking a step in the directions opposite to the gradients to make the model parameters a little bit better.\n",
        "\n",
        "> Learning rate - The size of the step we make when applying SGD to update the parameters of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyayD_Djdvuf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}